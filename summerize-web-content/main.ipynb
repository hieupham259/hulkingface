{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY valid\n",
      "LANGSMITH_API_KEY valid\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "else:\n",
    "  print(\"OPENAI_API_KEY valid\")\n",
    "\n",
    "if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
    "  os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key for LANGSMITH: \")\n",
    "else:\n",
    "  print(\"LANGSMITH_API_KEY valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "import requests\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped, now with links and paragraph extraction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "            # Extract all paragraph content\n",
    "            self.paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "            self.paragraph_text = \"\\n\".join(self.paragraphs)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "            self.paragraphs = []\n",
    "            self.paragraph_text = \"\"\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\"\n",
    "    \n",
    "    def get_paragraph_contents(self):\n",
    "        \"\"\"\n",
    "        Returns all paragraph content from the webpage combined\n",
    "        \"\"\"\n",
    "        return self.paragraph_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface = Website(\"https://huggingface.co/learn/llm-course/en/chapter1/4?fw=pt\")\n",
    "# Get all paragraph content\n",
    "paragraph_content = huggingface.get_paragraph_contents()\n",
    "# Or access the paragraphs list directly\n",
    "paragraphs = huggingface.paragraphs\n",
    "\n",
    "content = \"\"\n",
    "for paragraph in paragraphs:\n",
    "    content += paragraph\n",
    "    content += '\\n'\n",
    "\n",
    "content = content.split('We will dive into these families in more depth later on.')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summaried_content = summarizer(content)\n",
    "summaried_content = summaried_content[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this type of model develops a statistical understanding of the language it has been trained on . the general pretrained model then goes through a process calledtransfer learningorfine-tuning . this is because the output depends on the past and present inputs, but not the future ones .'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaried_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pipeline(\"translation\", model=\"VietAI/envit5-translation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at VietAI/envit5-translation.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'vi: Loại mô hình này phát triển một sự hiểu biết thống kê về ngôn ngữ mà nó đã được huấn luyện. mô hình được huấn luyện tổng quát sau đó trải qua một quá trình gọi là chuyển giao nhiễu học tập. điều này là do đầu ra phụ thuộc vào đầu vào quá khứ và hiện tại, nhưng không phụ thuộc vào tương lai.'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"VietAI/envit5-translation\")\n",
    "prompt = f\"translate English to Vietnamese: {summaried_content}\"\n",
    "translator(summaried_content, **{\"max_length\": 500})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
